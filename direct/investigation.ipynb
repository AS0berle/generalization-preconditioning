{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5a3de8d-0def-4b5e-b54b-b50ced368a12",
   "metadata": {},
   "source": [
    "# Direct Optimization of initial weights via Linear Programming\n",
    "\n",
    "For a fully connected feed forward neural net using RELU activation functions $f$, for a given input $x \\in D$ and parameter set $P = (W_1, W_2, ...)$ where each $W_i$ is the weight matrix for a layer of the network, then there exists matrix $A$ such that $f(x, P) = Ax$.\n",
    "\n",
    "Consider the following generalization loss function, defined as:\n",
    "\n",
    "$L_{gen} = (f(x, P) - f(x, P^*)) - (f(x^*, P) - f(x^*, P^*))$\n",
    "\n",
    "where $x^* = x + x'$ and $P^* = P + P'$ for some small perturbations $x', P'$ (applying a different perturbation to each weight matrix in $P'$)\n",
    "\n",
    "Then if perturbations $x', P'$ do not change the activation pattern of $f(x, P)$, $L_{gen}$ is equivalent to:\n",
    "\n",
    "$L_{gen} = Ax - (A + A')x - A(x+x') + (A+A')(x+x')$\n",
    "\n",
    "$ = Ax - Ax - A'x - Ax -Ax' + Ax + Ax' + A'x + A'x'$\n",
    "\n",
    "$ = A'x'$\n",
    "\n",
    "Therefore, we can obtain a small value of $L_{gen}$ by finding an initialization $P$ which has few activation patterns for the training data. Since the activation function is RELU, this is equivalent to reducing the amount of times RELU sets an element to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc237bd7-a106-4de7-a0ef-8941a805d8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" First goal: investigate the claim that having fewer RELU 0s is a good thing for an initialization\n",
    "\n",
    "    - Create a custom RELU subclass using PyTorch to count how often RELU 0s happen for initializations of a MLP\n",
    "    - Create many different initializations of a model and count the intial RELU 0s\n",
    "    - Train them all and compare trained model performance vs RELU 0 count\n",
    "\n",
    "    Ideally, will see a strongly negative correlation between number of RELU 0s and model performance\n",
    "\"\"\"\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c2cb620-1219-4d99-981b-09c667e8ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluCount(nn.ReLU):\n",
    "    \"\"\"\n",
    "        Intended to behave identically to nn.ReLU, except for the zero counting.\n",
    "        To disable zero counting, set count_zeros to False.\n",
    "    \"\"\"\n",
    "    def __init__(self, inplace: bool = False):\n",
    "        super().__init__(inplace)\n",
    "        self.zero_count = 0\n",
    "        self.count_zeros = False\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        true_result = F.relu(input, inplace=self.inplace)\n",
    "        if self.count_zeros == True:\n",
    "            for entry in torch.flatten(true_result):\n",
    "                if entry.item() == 0:\n",
    "                    self.zero_count += 1\n",
    "        return true_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fbcb9a6-833f-47e1-8616-92477f6f7f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_MEAN = 0.1307\n",
    "MNIST_STD = 0.3081\n",
    "\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "\n",
    "\n",
    "train_loader = DataLoader(MNIST('images/', train=True, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize((MNIST_MEAN,), (MNIST_STD,))\n",
    "                               ])),\n",
    "                          batch_size=batch_size_train, shuffle=True, pin_memory=False)\n",
    "\n",
    "test_loader = DataLoader(MNIST('images/', train=False, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize((MNIST_MEAN,), (MNIST_STD,))\n",
    "                               ])),\n",
    "                         batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef7ede0b-4efe-44d6-afc6-08d074e3f135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 784])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "example_data.shape\n",
    "\n",
    "flat = torch.flatten(example_data, 1)\n",
    "flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60d0121d-2204-45fd-836f-2b1d4de2b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistReluCount(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu_count = ReluCount()\n",
    "        # mnist images are 1x28x28, so flattened they will have a length of 28*28=784\n",
    "        self.fc1 = nn.Linear(784, 750)\n",
    "        self.fc2 = nn.Linear(750, 320)\n",
    "        self.fc3 = nn.Linear(320, 50)\n",
    "        self.fc4 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten batches 2D images to 1D vectors\n",
    "        x = self.relu_count(self.fc1(x))\n",
    "        x = self.relu_count(self.fc2(x))\n",
    "        x = self.relu_count(self.fc3(x))\n",
    "        x = self.relu_count(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29385bcd-51b0-4754-8759-4baad4bd12bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    total_loss = 0\n",
    "    batches = 0\n",
    "    \n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        \n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "        batches += 1\n",
    "        \n",
    "    return total_loss / batches\n",
    "\n",
    "def count_relu_0s(dataloader, model: ReluCount):\n",
    "    model.eval()\n",
    "    model.relu_count.count_zeros = True\n",
    "    for (X, y) in dataloader:\n",
    "        model(X)\n",
    "    model.relu_count.count_zeros = False\n",
    "    return model.relu_count.zero_count\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for (X, y) in dataloader:\n",
    "            out = model(X)\n",
    "            test_loss += loss_fn(out, y).item()\n",
    "            pred = out.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(y.data.view_as(pred)).sum()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    accuracy = 100.0 * correct / len(dataloader.dataset)\n",
    "    print(f\"Avg loss: {test_loss:>8f}\")\n",
    "    print(f\"Accuracy: {correct}/{len(dataloader.dataset)} = {accuracy}\")\n",
    "    print()\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11f4e612-745f-47ce-b6cb-be80543e91e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .01\n",
    "epochs = 5\n",
    "model = MnistReluCount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38115b91-04f0-4fac-b3f1-878152b44021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting initial ReLU 0s took 31.979676246643066 seconds\n",
      "5779271\n"
     ]
    }
   ],
   "source": [
    "# get RELU 0 count\n",
    "start_t = time.time()\n",
    "model_zeros = count_relu_0s(test_loader, model)\n",
    "print(f\"Counting initial ReLU 0s took {time.time() - start_t} seconds\")\n",
    "print(model_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc876afb-16d4-451b-9c2e-aecb5b85b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02ca9aca-7ec7-4d15-b1d6-2e06d0e943dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Avg loss: 0.908930\n",
      "Accuracy: 7288/10000 = 72.87999725341797\n",
      "\n",
      "Epoch: 2\n",
      "Avg loss: 0.728550\n",
      "Accuracy: 7505/10000 = 75.05000305175781\n",
      "\n",
      "Epoch: 3\n",
      "Avg loss: 0.664025\n",
      "Accuracy: 7619/10000 = 76.19000244140625\n",
      "\n",
      "Epoch: 4\n",
      "Avg loss: 0.628251\n",
      "Accuracy: 7689/10000 = 76.88999938964844\n",
      "\n",
      "Epoch: 5\n",
      "Avg loss: 0.607580\n",
      "Accuracy: 7725/10000 = 77.25\n",
      "\n",
      "Finished after 55.181816816329956 seconds\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "\n",
    "start_t = time.time()\n",
    "for t in range(epochs):\n",
    "    \n",
    "    train_losses.append(train_loop(train_loader, model, loss_function, optimizer))\n",
    "    if True:\n",
    "        print(f\"Epoch: {t+1}\")\n",
    "        test_losses.append(test_loop(test_loader, model, loss_function))\n",
    "end_t = time.time()\n",
    "print(f\"Finished after {end_t - start_t} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3a7d79-1503-4ce6-b914-40aedfbf7c02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
