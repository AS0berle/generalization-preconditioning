{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5a3de8d-0def-4b5e-b54b-b50ced368a12",
   "metadata": {},
   "source": [
    "# Direct Optimization of initial weights via Linear Programming\n",
    "\n",
    "For a fully connected feed forward neural net using RELU activation functions $f$, for a given input $x \\in D$ and parameter set $P = (W_1, W_2, ...)$ where each $W_i$ is the weight matrix for a layer of the network, then there exists matrix $A$ such that $f(x, P) = Ax$.\n",
    "\n",
    "Consider the following generalization loss function, defined as:\n",
    "\n",
    "$L_{gen} = (f(x, P) - f(x, P^*)) - (f(x^*, P) - f(x^*, P^*))$\n",
    "\n",
    "where $x^* = x + x'$ and $P^* = P + P'$ for some small perturbations $x', P'$ (applying a different perturbation to each weight matrix in $P'$)\n",
    "\n",
    "Then if perturbations $x', P'$ do not change the activation pattern of $f(x, P)$, $L_{gen}$ is equivalent to:\n",
    "\n",
    "$L_{gen} = Ax - (A + A')x - A(x+x') + (A+A')(x+x')$\n",
    "\n",
    "$ = Ax - Ax - A'x - Ax -Ax' + Ax + Ax' + A'x + A'x'$\n",
    "\n",
    "$ = A'x'$\n",
    "\n",
    "Therefore, we can obtain a small value of $L_{gen}$ by finding an initialization $P$ which has few activation patterns for the training data. Since the activation function is RELU, this is equivalent to reducing the amount of times RELU sets an element to 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
