{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d9e8d1a-cd3e-4c15-8e4a-3757f454444a",
   "metadata": {},
   "source": [
    "# Training a lot of models initialized via LP method\n",
    "\n",
    "There will be some amount of code duplication between this and investigation.ipynb, and the code is *rough* in places. For now, just getting it all to run is good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddfcd435-d744-4a2f-95c0-e756d3887050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "from direct_helpers import *\n",
    "\n",
    "from scipy.optimize import linprog\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f43bb767-c61c-480f-891e-1ac7d7a27f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_lp_model():\n",
    "    \"\"\"\n",
    "    Get an MnistReluCountModel with weights initialized by LP method\n",
    "\n",
    "    This isn't the prettiest, but it'll do.\n",
    "    \"\"\"\n",
    "    loader = DataLoader(MNIST('images/', train=True, download=True,\n",
    "                                   transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.ToTensor(),\n",
    "                                   torchvision.transforms.Normalize((MNIST_MEAN,), (MNIST_STD,))\n",
    "                                   ])),\n",
    "                              batch_size=1, shuffle=True, pin_memory=False)\n",
    "    opt_model = MnistReluCountModel()\n",
    "    in_size = 784\n",
    "    out_size = 750\n",
    "    \n",
    "    c_initials= []\n",
    "    for _ in range(out_size):\n",
    "        c_initials.append(torch.flatten(get_avg_MNIST(loader=loader, sample_size=1), 0) * -1) # linprog minimizes, we want to maximize\n",
    "    opt_model.fc1.weight = linprog_parameter(c_initials, in_size)\n",
    "\n",
    "    \n",
    "    in_size = out_size\n",
    "    out_size = 320\n",
    "    \n",
    "    relu = nn.ReLU()\n",
    "    c2_samples = []\n",
    "    for _ in range(out_size):\n",
    "        rand_sample = torch.flatten(get_avg_MNIST(loader=loader, sample_size=1), 0) # get a random sample\n",
    "        sample = relu(opt_model.fc1(rand_sample)) * -1 # pass through layer 1, then scale by -1 for LP\n",
    "        c2_samples.append(sample)\n",
    "    opt_model.fc2.weight = linprog_parameter(c2_samples, in_size)\n",
    "\n",
    "    in_size = out_size\n",
    "    out_size = 50\n",
    "    \n",
    "    c3_samples = []\n",
    "    for _ in range(out_size):\n",
    "        rand_sample = torch.flatten(get_avg_MNIST(loader=loader, sample_size=1), 0) # get a random sample\n",
    "        sample = relu(opt_model.fc1(rand_sample)) # pass through layer 1\n",
    "        sample = relu(opt_model.fc2(sample)) * -1 # pass through layer 2, then scale by -1 for LP\n",
    "        c3_samples.append(sample)\n",
    "    opt_model.fc3.weight = linprog_parameter(c3_samples, in_size)\n",
    "\n",
    "    in_size = out_size\n",
    "    out_size = 10\n",
    "    \n",
    "    c4_samples = []\n",
    "    for _ in range(out_size):\n",
    "        rand_sample = torch.flatten(get_avg_MNIST(loader=loader, sample_size=1), 0) # get a random sample\n",
    "        sample = relu(opt_model.fc1(rand_sample)) # pass through layer 1\n",
    "        sample = relu(opt_model.fc2(sample)) # pass through layer 2\n",
    "        sample = relu(opt_model.fc3(sample)) * -1 # pass through layer 3, then scale by -1 for LP\n",
    "        c4_samples.append(sample)\n",
    "    opt_model.fc4.weight = linprog_parameter(c4_samples, in_size)\n",
    "\n",
    "    return opt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dd42fa3-62b1-45a3-b619-286a76d5b536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    total_loss = 0\n",
    "    batches = 0\n",
    "    \n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        \n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "        batches += 1\n",
    "        \n",
    "    return total_loss / batches\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, quiet=False):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for (X, y) in dataloader:\n",
    "            out = model(X)\n",
    "            test_loss += loss_fn(out, y).item()\n",
    "            pred = out.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(y.data.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    accuracy = 100.0 * correct / len(dataloader.dataset)\n",
    "    if not quiet:\n",
    "        print(f\"Avg loss: {test_loss:>8f}\")\n",
    "        print(f\"Accuracy: {correct}/{len(dataloader.dataset)} = {accuracy}\")\n",
    "        print()\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "294ed0ed-6e77-4a8f-bc5a-6b1b5cb46925",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingResults():\n",
    "    def __init__(self, train_losses, test_losses, test_accs, init_zeros, trained_zeros):\n",
    "        self.train_losses = train_losses\n",
    "        self.test_losses = test_losses\n",
    "        self.test_accs = test_accs\n",
    "        self.initial_zeros = init_zeros\n",
    "        self.trained_zeros = trained_zeros\n",
    "\n",
    "    def __repr__(self):\n",
    "        init_zeros =    f\"Initial 0 count:        {self.initial_zeros}\\n\"\n",
    "        trained_zeros = f\"0 count after training: {self.trained_zeros}\\n\"\n",
    "        train = f\"Train losses:    {self.train_losses}\\n\"\n",
    "        test =  f\"Test losses:     {self.test_losses}\\n\"\n",
    "        accs =  f\"Test Accuracies: {self.test_accs}\\n\"\n",
    "        return init_zeros + trained_zeros + train + test + accs\n",
    "\n",
    "\n",
    "def train_lp_mnist_model(train_loader:DataLoader, test_loader:DataLoader, epochs=5, lr=.01):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    \n",
    "    model = get_mnist_lp_model()\n",
    "    initial_zeros = count_relu_0s(test_loader, model)\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    for t in range(epochs):\n",
    "        train_loss = train_loop(train_loader, model, loss_function, optimizer)\n",
    "        \n",
    "        test_loss, test_acc = test_loop(test_loader, model, loss_function, quiet=True)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "    trained_zeros = count_relu_0s(test_loader, model)\n",
    "    results = TrainingResults(train_losses, test_losses, test_accs, \n",
    "                              initial_zeros, trained_zeros)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99789521-abb6-41ae-9675-d9db9a088865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollectedResults():\n",
    "    def __init__(self, results):\n",
    "        self.all_results = results\n",
    "        # should convert all results into ndarrays internally\n",
    "        # should also allow construction by reading files\n",
    "\n",
    "    def write_train_losses(self, file_path:str=\"train_losses.csv\"):\n",
    "        pass\n",
    "\n",
    "    def write_test_losses(self, file_path:str=\"test_losses.csv\"):\n",
    "        pass\n",
    "\n",
    "    def write_test_accs(self, file_path:str=\"test_accs.csv\"):\n",
    "        pass\n",
    "\n",
    "    def write_zero_counts(self, file_path:str=\"zero_counts.csv\"):\n",
    "        pass\n",
    "        \n",
    "    def write_to_default_files(self):\n",
    "        self.write_train_losses()\n",
    "        self.write_test_losses()\n",
    "        self.write_test_accs()\n",
    "        self. write_zero_counts()\n",
    "\n",
    "\n",
    "def train_many(num_to_train=2, epochs=5, lr=.01):\n",
    "    train_loader = DataLoader(MNIST('images/', train=True, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize((MNIST_MEAN,), (MNIST_STD,))\n",
    "                               ])),\n",
    "                          batch_size=batch_size_train, shuffle=True, pin_memory=False)\n",
    "\n",
    "    test_loader = DataLoader(MNIST('images/', train=False, download=True,\n",
    "                                   transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.ToTensor(),\n",
    "                                   torchvision.transforms.Normalize((MNIST_MEAN,), (MNIST_STD,))\n",
    "                                   ])),\n",
    "                             batch_size=batch_size_test, shuffle=True)\n",
    "    results = []\n",
    "    for _ in range(num_to_train):\n",
    "        results.append(train_lp_mnist_model(train_loader, test_loader, epochs, lr))\n",
    "\n",
    "    collected_results = CollectedResults(results)\n",
    "    return collected_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6b50602-4aff-464a-97f3-6eee76fbf429",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "\n",
    "train_loader = DataLoader(MNIST('images/', train=True, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize((MNIST_MEAN,), (MNIST_STD,))\n",
    "                               ])),\n",
    "                          batch_size=batch_size_train, shuffle=True, pin_memory=False)\n",
    "\n",
    "test_loader = DataLoader(MNIST('images/', train=False, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize((MNIST_MEAN,), (MNIST_STD,))\n",
    "                               ])),\n",
    "                         batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4da29f27-5c87-45ec-817a-2d63048be924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 123.58857035636902 seconds!\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "results = train_lp_mnist_model(train_loader, test_loader)\n",
    "stop = time.time()\n",
    "print(f\"Took {stop - start} seconds!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d24e578f-77ee-4fd4-a887-6fef820f84c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial 0 count:        518531\n",
      "0 count after training: 1185827\n",
      "Train losses:    [1.3455590092614769, 0.7725152661487746, 0.7070979224339223, 0.6758331570988779, 0.6539220727646529]\n",
      "Test losses:     [0.8996824741363525, 0.726496160030365, 0.7187384366989136, 0.67099609375, 0.6722821354866028]\n",
      "Test Accuracies: [73.91, 81.5, 81.72, 83.96, 84.25]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa1dcee8-8590-4d28-b93a-bbc84a96125d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 224.89110589027405 seconds!\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "cr = train_many()\n",
    "stop = time.time()\n",
    "print(f\"Took {stop - start} seconds!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c560e045-44f9-4b69-8eff-f367afa06171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial 0 count:        433476\n",
      "0 count after training: 1367100\n",
      "Train losses:    [1.3062069416681588, 1.0145195541478425, 0.8343594954339172, 0.7296054085243994, 0.6997699674957597]\n",
      "Test losses:     [1.0711086869239808, 0.9713546991348266, 0.7518130004405975, 0.7102375149726867, 0.6880674421787262]\n",
      "Test Accuracies: [60.44, 62.68, 72.06, 73.18, 73.59]\n",
      "\n",
      "\n",
      "Initial 0 count:        483937\n",
      "0 count after training: 1196600\n",
      "Train losses:    [1.5089067071358533, 1.1702006868462065, 1.1165456987901536, 1.0902361566705236, 1.0724759242936237]\n",
      "Test losses:     [1.1998318195343018, 1.1066844463348389, 1.1232999205589294, 1.0551402986049652, 1.044065886735916]\n",
      "Test Accuracies: [64.5, 66.2, 65.95, 67.62, 67.8]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in cr.all_results:\n",
    "    print(result)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
