{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb7041cc-a3d3-4e3d-b28a-3c35babe9119",
   "metadata": {},
   "source": [
    "# Initial Exploration\n",
    "\n",
    "## Problem description\r\n",
    "\r\n",
    "Let f(x, P) be a neural network with fixed structure that takes as input a feature vector x and computes a real-valued output based on learnable parameters P.  Suppose that in response to a training example, the parameters are updated to P'.  The difference between the value of the function before and after the update is:\r\n",
    "\r\n",
    "f(x, P) - f(x, P')\r\n",
    "\r\n",
    "We would like a network that generalizes in the neighborhood of x.  That is, if x' is an input vector “close” to x, we would like the change in f due to the change in P at x to be similar in sign and magnitude to the change in f at x' due to the change in P.  The latter quantity can be written:\r\n",
    "\r\n",
    "f(x', P) - f(x', P')\r\n",
    "\r\n",
    "They are similar if the following loss is _all:\r\n",
    "\r\n",
    "L\\_gen = ((f(x, P) - f(x, P')) - (f(x', P) - f(x', P'))^2\r\n",
    "\r\n",
    "If I update the parameters from P to P', the change in f should be similar in the neighborhood of x.  As |x - x'| gets larger, the neighborhood of the generalization increases.\r\n",
    "\r\n",
    "Our goal is to pre-train a network using unlabeled data to get into a part of the parameter space such that generalization during pre-training is broad, thereby leading to faster convergence.  Consider the following algorithmic approach to this idea.\r\n",
    "\r\n",
    "\r\n",
    "```\r\n",
    "Input: X = {x1, x2, , xn}, neural network f, x, P\r\n",
    "x is the standard deviation of the noise applied to x\r\n",
    "p is the standard deviation of the noise applied to P\r\n",
    "\r\n",
    "While not converged\r\n",
    "  Choose x from X at random\r\n",
    "  x’ = x + N(0, x)\r\n",
    "  P’ = P + N(0, P)\r\n",
    "  Compute derivative of  Lgen wrt P\r\n",
    "  Update P using gradient descent\r\n",
    "\r\n",
    "Return P\r\n",
    "```gradient descent\r\n",
    "\r\n",
    "Return P\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0c574f4-628d-4568-81eb-c13c162e9289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99c96a56-6371-40e4-aebc-79ba8af5a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Linear(1, 64),\n",
    "          nn.Sigmoid(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.Sigmoid(),\n",
    "          nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626b4dd1-8a2b-467d-9c65-b115b7233e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28dfbddb-9b4d-4f57-bf06-ce7e0079490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train iterations\n",
    "sin_model = TinyModel()\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(sin_model.parameters(), lr=1e-4)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
